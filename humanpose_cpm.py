# -*- coding: utf-8 -*-
"""humanPose.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1baOLBScNT0bLQaWtfkPQdCg-PQDMeqq9
"""

# 1. Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 2. Install required packages
!pip install pycocotools
!pip install albumentations

# 3. Download COCO dataset (you can choose train2017 or val2017)
!wget http://images.cocodataset.org/zips/val2017.zip
!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip

# 4. Unzip the files
!unzip val2017.zip
!unzip annotations_trainval2017.zip

!pwd
!ls

!pip install pycocotools opencv-python matplotlib
!pip install --upgrade sympy

!cat annotations/

# Commented out IPython magic to ensure Python compatibility.
# %cd annotations/

!pwd
!ls

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pycocotools.coco import COCO
from torch.utils.data import Dataset, DataLoader
#from tensorflow.keras import Model
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, Input, ReLU, MaxPooling2D, Concatenate
import tensorflow as tf
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import albumentations as A
from albumentations.pytorch import ToTensorV2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import Sequence

# CPM Model Definition
def cpm_model(input_shape=(3, 256, 256), num_keypoints=17, stages=3):
    inputs = Input(shape=input_shape)

    # Initial convolutional block
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)
    print("Shape after first Conv2D:", x.shape)  # Print shape
    x = MaxPooling2D((2, 1))(x)
    print("Shape after first MaxPooling2D:", x.shape)  # Print shape
    x = Conv2D(128, (3, 3), padding='same', activation='relu', strides=(2, 1))(x)
    print("Shape after second Conv2D:", x.shape)  # Print shape
    #x = MaxPooling2D((2, 2))(x)
    #print("Shape after second MaxPooling2D:", x.shape)  # Print shape
    shared_features = Conv2D(256, (3, 3), padding='same', activation='relu')(x)
    print("Shape of shared_features:", shared_features.shape)  # Add print statement


    # CPM stages
    stage_output = Conv2D(num_keypoints, (1, 1), padding='same', activation='sigmoid')(shared_features)
    for _ in range(stages - 1):
        stage_input = Concatenate()([shared_features, stage_output])
        stage_output = Conv2D(128, (3, 3), padding='same', activation='relu')(stage_input)
        stage_output = Conv2D(num_keypoints, (1, 1), padding='same', activation='sigmoid')(stage_output)  # Output heatmaps

    model = Model(inputs, stage_output)
    return model

# Data Preparation and Loading
class COCOSinglePersonDataset(Dataset):   # (Sequence)
    #def __init__(self, coco_path, annotation_path, batch_size=16, input_size=(256, 256)):
    def __init__(self, coco_path, annotation_path, transform=None):
        self.coco = COCO(annotation_path)
        self.coco_path = coco_path
        self.transform = transform
        #self.input_size = input_size
        #self.batch_size = batch_size

        # COCO Keypoint names in order
        self.keypoint_names = [
            'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
            'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
            'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
            'left_knee', 'right_knee', 'left_ankle', 'right_ankle'
        ]




        # Get single person images
        self.img_ids = []
        self.ann_ids = []
        self.cat_ids = self.coco.getCatIds(catNms=['person'])
        img_ids = self.coco.getImgIds(catIds=self.cat_ids)

        # Filter for single person images with good keypoints
        for img_id in img_ids:
            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.cat_ids)
            if len(ann_ids) == 1:  # Single person
                ann = self.coco.loadAnns(ann_ids[0])[0]
                if sum(1 for v in ann['keypoints'][2::3] if v > 0) >= 5:  # At least 5 visible keypoints
                    self.img_ids.append(img_id)
                    self.ann_ids.append(ann_ids[0])

        print(f"Found {len(self.img_ids)} single person images with good keypoints")

    #def __len__(self):
     #   return len(self.img_ids) // self.batch_size

    def __len__(self):
        return len(self.img_ids)  # Return the total number of samples

    def __getitem__(self, idx):
        img_id = self.img_ids[idx]
        ann_id = self.ann_ids[idx]

        # Load annotation
        ann = self.coco.loadAnns(ann_id)[0]

        # Load image
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.coco_path, img_info['file_name'])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Get keypoints
        keypoints = np.array(ann['keypoints']).reshape(-1, 3)

        if self.transform:
            augmented = self.transform(image=image, keypoints=keypoints[:, :2])
            image = augmented['image']
            keypoints = augmented['keypoints']

        # Convert keypoints to heatmap (adjust size based on model output)
        heatmap = self.generate_heatmaps(keypoints, image.shape[:2])  # Use image shape
        epsilon = 1e-8  # A small value to prevent division by zero
        heatmap = heatmap / (np.max(heatmap) + epsilon) #heatmap / np.max(heatmap)  # Normalize heatmap

        return image, heatmap  # Return a single sample
    '''
        batch_ids = self.img_ids[idx * self.batch_size:(idx + 1) * self.batch_size]
        images = []
        heatmaps = []

        for img_id in batch_ids:
            img_info = self.coco.loadImgs(img_id)[0]
            img_path = os.path.join(self.coco_path, img_info['file_name'])
            image = cv2.imread(img_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = cv2.resize(image, self.input_size)

            ann = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))[0]
            keypoints = np.array(ann['keypoints']).reshape(-1, 3)
            heatmap = self.generate_heatmaps(keypoints, self.input_size)

            images.append(image / 255.0)  # Normalize to [0, 1]
            heatmaps.append(heatmap)

        return np.array(images), np.array(heatmaps)
    '''

    def generate_heatmaps(self, keypoints, input_size, output_size=(1, 256), sigma=2):  # sigma=2 added
        heatmaps = np.zeros((output_size[0], output_size[1], 17))  # 17 keypoints
        scale_x = output_size[1] / input_size[1]
        scale_y = output_size[0] / input_size[0]

        #for idx, (x, y, v) in enumerate(keypoints):
        for idx, kp in enumerate(keypoints):  # Iterate through keypoints
          if len(kp) == 3:  # Check if keypoint has x, y, and v
              x, y, v = kp
              if v > 0:  # Visible keypoint
                  x_resized = x * scale_x
                  y_resized = y * scale_y
                  heatmaps[..., idx] = self.gaussian_heatmap((x_resized, y_resized), output_size, sigma)

        return heatmaps

    #def gaussian_heatmap(self, center, size, sigma=4):
     #   grid_x, grid_y = np.meshgrid(np.arange(size[1]), np.arange(size[0]))
      #  heatmap = np.exp(-((grid_x - center[0])**2 + (grid_y - center[1])**2) / (2 * sigma**2))
       # return heatmap
    def gaussian_heatmap(self, center, size, sigma=2):
        grid_x, grid_y = np.meshgrid(np.arange(size[1]), np.arange(size[0]))
        heatmap = np.exp(-((grid_x - center[0])**2 + (grid_y - center[1])**2) / (2 * sigma**2))
        return heatmap

# Data Transformations
transform = A.Compose([
    A.Resize(256, 256),  # Resize images
    A.HorizontalFlip(p=0.5),  # Example augmentation
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize
    ToTensorV2()
], keypoint_params=A.KeypointParams(format='xy'))

# Training and Validation Data
train_dataset = COCOSinglePersonDataset(
    coco_path='/content/val2017',
    annotation_path='/content/annotations/person_keypoints_val2017.json', # was val2017
    transform=transform #batch_size=16
)
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Use DataLoader

# Validation dataset
val_dataset = COCOSinglePersonDataset(
    coco_path='/content/val2017',  #  validation image path
    annotation_path='/content/annotations/person_keypoints_val2017.json',  #
    transform=transform
)
val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)

# Model Compilation
model = cpm_model(input_shape=(3, 256, 256), num_keypoints=17, stages=3)
model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['accuracy'])

'''
# Training the Model
model.fit(
    train_dataset,
    epochs=10,
    steps_per_epoch=len(train_dataset),
    verbose=1
)
'''
# Training the Model (with validation data and loss printing)
num_epochs = 10
for epoch in range(num_epochs):
    for batch_idx, (images, heatmaps) in enumerate(train_dataloader):
        images = tf.convert_to_tensor(images.numpy()) # Convert images to TensorFlow tensor
        heatmaps = tf.convert_to_tensor(heatmaps.numpy()) # Convert heatmaps to TensorFlow tensor
        # Forward pass
        with tf.GradientTape() as tape:
            outputs = model(images)  # Get output from all stages
      #      print("Shape of model output:", outputs.shape)  # Add print statement
            '''
            # Calculate loss with intermediate supervision
            loss = 0
            for stage_output in outputs:
                loss += tf.reduce_mean(tf.keras.losses.MeanSquaredError()(stage_output, heatmaps))
            loss /= len(outputs)  # Average loss over stages
            '''
            # Calculate loss using the final stage output only
            loss = tf.reduce_mean(tf.keras.losses.MeanSquaredError()(outputs, heatmaps))
            # Remove loop and intermediate supervision (loss calculation)

        # Backpropagation and optimization
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer = Adam(learning_rate=1e-3) # define optimizer before using it
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        # Print loss (every 100 batches)
        if batch_idx % 100 == 0:
            print(f"Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx+1}, Loss: {loss.numpy():.4f}")

    # Validation (every epoch)
    val_loss = 0
    for val_batch_idx, (val_images, val_heatmaps) in enumerate(val_dataloader):
        val_images = tf.convert_to_tensor(val_images.numpy()) # Convert images to TensorFlow tensor
        val_heatmaps = tf.convert_to_tensor(val_heatmaps.numpy()) # Convert heatmaps to TensorFlow tensor
        val_outputs = model(val_images)
        batch_val_loss = 0
        for stage_output in val_outputs:
            batch_val_loss += tf.reduce_mean(tf.keras.losses.MeanSquaredError()(stage_output, val_heatmaps))
        val_loss += batch_val_loss / len(val_outputs)
    val_loss /= len(val_dataloader)
    print(f"Epoch: {epoch+1}/{num_epochs}, Validation Loss: {val_loss.numpy():.4f}")

'''
# Visualizing Predictions
def visualize_predictions(image, true_heatmaps, pred_heatmaps):
    plt.imshow(image)
    for i in range(17):  # Iterate through 17 keypoints
        true_y, true_x = np.unravel_index(np.argmax(true_heatmaps[..., i]), true_heatmaps[..., i].shape)
        pred_y, pred_x = np.unravel_index(np.argmax(pred_heatmaps[..., i]), pred_heatmaps[..., i].shape)
        plt.plot(true_x, true_y, 'go')  # Ground truth
        plt.plot(pred_x, pred_y, 'ro')  # Prediction
    plt.show()
'''

# Test
test_image, test_heatmaps = train_dataset[0]
#pred_heatmaps = model.predict(np.expand_dims(test_image, axis=0))  # Add batch dimension
pred_heatmaps = model.predict(test_image[None, ...]) # added batch dimension
#pred_heatmaps = model.predict(test_image)
pred_heatmaps = pred_heatmaps[0]  # Remove batch dimension

# Visualizing Predictions (with corrected indexing)
def visualize_predictions(image, true_heatmaps, pred_heatmaps):
    plt.figure(figsize=(8, 8))
    # Convert the image to NumPy array and permute the dimensions
    image_np = image.permute(1, 2, 0).cpu().numpy()  # Change to (256, 256, 3) and move to CPU
    plt.imshow(image_np)
    for i in range(17):
        true_y, true_x = np.unravel_index(np.argmax(true_heatmaps[:, :, i]), true_heatmaps[:, :, i].shape)
        pred_y, pred_x = np.unravel_index(np.argmax(pred_heatmaps[:, :, i]), pred_heatmaps[:, :, i].shape)
        plt.plot(true_x * 4, true_y * 4, 'go')  # Ground truth (adjust for original image size)
        plt.plot(pred_x * 4, pred_y * 4, 'ro')  # Prediction (adjust for original image size)
    plt.show()

#visualize_predictions(test_image[0], test_heatmaps[0], pred_heatmaps[0])
visualize_predictions(test_image, test_heatmaps, pred_heatmaps)

'''
class COCOSinglePersonDataset(Dataset):
    def __init__(self, coco_path, annotation_path, transform=None):
        self.coco = COCO(annotation_path)
        self.coco_path = coco_path
        self.transform = transform

        # COCO Keypoint names in order
        self.keypoint_names = [
            'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
            'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
            'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
            'left_knee', 'right_knee', 'left_ankle', 'right_ankle'
        ]

        # Color map for different body parts
        self.colors = {
            'nose': 'red',
            'eyes': 'blue',
            'ears': 'green',
            'shoulders': 'yellow',
            'elbows': 'purple',
            'wrists': 'orange',
            'hips': 'cyan',
            'knees': 'magenta',
            'ankles': 'brown'
        }

        # Define connections between keypoints for skeletal visualization
        self.skeleton = [
            ('left_shoulder', 'right_shoulder'),
            ('left_shoulder', 'left_elbow'),
            ('right_shoulder', 'right_elbow'),
            ('left_elbow', 'left_wrist'),
            ('right_elbow', 'right_wrist'),
            ('left_shoulder', 'left_hip'),
            ('right_shoulder', 'right_hip'),
            ('left_hip', 'right_hip'),
            ('left_hip', 'left_knee'),
            ('right_hip', 'right_knee'),
            ('left_knee', 'left_ankle'),
            ('right_knee', 'right_ankle')
        ]

        # Filter for single person
        self.img_ids = []
        self.ann_ids = []

        # Get person category id
        self.cat_ids = self.coco.getCatIds(catNms=['person'])

        # Get all image ids with people
        all_img_ids = self.coco.getImgIds(catIds=self.cat_ids)


        # Filter for single person images with good keypoints
        for img_id in all_img_ids:
            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.cat_ids)
            if len(ann_ids) == 1:  # Single person
                ann = self.coco.loadAnns(ann_ids[0])[0]
                # Check if enough keypoints are visible
                num_visible_kpts = sum(1 for v in ann['keypoints'][2::3] if v > 0)
                if num_visible_kpts >= 5:  # At least 5 visible keypoints
                    self.img_ids.append(img_id)
                    self.ann_ids.append(ann_ids[0])

        print(f"Found {len(self.img_ids)} single person images with good keypoints")

        def cpm_model(input_shape=(256, 256, 3), num_keypoints=17, stages=3):
            inputs = Input(shape=input_shape)

            # Initial convolutional block
            x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)
            x = MaxPooling2D((2, 2))(x)
            x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
            x = MaxPooling2D((2, 2))(x)
            shared_features = Conv2D(256, (3, 3), padding='same', activation='relu')(x)

            # CPM stages
            stage_output = Conv2D(num_keypoints, (1, 1), padding='same', activation='sigmoid')(shared_features)
            for _ in range(stages - 1):
                stage_input = Concatenate()([shared_features, stage_output])
                stage_output = Conv2D(128, (3, 3), padding='same', activation='relu')(stage_input)
                stage_output = Conv2D(num_keypoints, (1, 1), padding='same', activation='sigmoid')(stage_output)

            model = Model(inputs, stage_output)
        return model

    # Instantiate the model
    model = cpm_model()
    model.summary()

    def __len__(self):
          return len(self.img_ids)

    def __getitem__(self, idx):
        img_id = self.img_ids[idx]
        ann_id = self.ann_ids[idx]

        # Load annotation
        ann = self.coco.loadAnns(ann_id)[0]

        # Load image
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.coco_path, img_info['file_name'])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Get keypoints
        keypoints = np.array(ann['keypoints']).reshape(-1, 3)

        if self.transform:
            augmented = self.transform(image=image, keypoints=keypoints[:, :2])
            image = augmented['image']
            keypoints = augmented['keypoints']

        return image, keypoints

    # visualize
def visualize_sample(dataset, idx):
    image, keypoints = dataset[idx]

    plt.figure(figsize=(12, 8))
    plt.imshow(image)

    # Plot keypoints with labels and colors
    for i, (x, y, v) in enumerate(keypoints):
        if v > 0:  # Keypoint is visible
            keypoint_name = dataset.keypoint_names[i]
            # Get color based on body part
            color = dataset.colors.get(keypoint_name.split('_')[-1], 'white')
            plt.plot(x, y, 'o', color=color, markersize=8)
            plt.text(x+5, y+5, keypoint_name, color='white',
                    bbox=dict(facecolor='black', alpha=0.7))

    # Draw skeleton lines
    for (kp1_name, kp2_name) in dataset.skeleton:
        idx1 = dataset.keypoint_names.index(kp1_name)
        idx2 = dataset.keypoint_names.index(kp2_name)
        if keypoints[idx1][2] > 0 and keypoints[idx2][2] > 0:  # Both keypoints visible
            plt.plot([keypoints[idx1][0], keypoints[idx2][0]],
                    [keypoints[idx1][1], keypoints[idx2][1]],
                    'w-', alpha=0.7)

    plt.title(f'Sample {idx} with Labeled Keypoints')
    plt.axis('off')
    plt.show()

    # Print visibility information
    print("\nKeypoint Visibility:")
    for i, (x, y, v) in enumerate(keypoints):
        status = "Visible" if v > 0 else "Not Visible"
        print(f"{dataset.keypoint_names[i]}: {status}")


# Create dataset
dataset = COCOSinglePersonDataset(
    coco_path='/content/val2017',
    annotation_path='/content/annotations/person_keypoints_val2017.json'
)

# Visualize samples
for i in range(3):  # Show 3 samples
    visualize_sample(dataset, i)
'''

